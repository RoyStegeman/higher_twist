{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from validphys.api import API\n",
    "from validphys.theorycovariance.higher_twist_functions import compute_deltas_pc\n",
    "\n",
    "from ht_plot_utlis import plot_covmat_heatmap\n",
    "from ht_utlis import TCM, HTset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitname = \"250125-ac-01-pc-jet\"\n",
    "thcovmat_dict = API.fit(fit=fitname).as_input()[\"theorycovmatconfig\"]\n",
    "pc_parameters = thcovmat_dict['pc_parameters']\n",
    "covmat_pdf = thcovmat_dict['pdf']\n",
    "pc_included_prcs = thcovmat_dict['pc_included_procs']\n",
    "pc_excluded_exps = thcovmat_dict['pc_excluded_exps']\n",
    "fitpath = API.fit(fit=fitname).path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the index used to label the parameters used to parameterise the higher twist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_names = []\n",
    "ht_nodes = []\n",
    "x_nodes = {}\n",
    "beta_tilde = []\n",
    "for ht in pc_parameters:\n",
    "  beta_tilde.append(ht['yshift'])\n",
    "  x_nodes[ht['ht']] = ht['nodes']\n",
    "  for idx_node in range(len(ht['yshift'])):\n",
    "    ht_names.append(ht['ht'])\n",
    "    ht_nodes.append(ht['ht'] + f\"({idx_node})\")\n",
    "\n",
    "beta_tilde = np.concatenate(beta_tilde)\n",
    "\n",
    "# Compute beta_tilde according to the 5pt prescription\n",
    "tmp_mat = np.zeros(shape=(len(beta_tilde), len(beta_tilde)))\n",
    "np.fill_diagonal(tmp_mat, beta_tilde)\n",
    "\n",
    "\n",
    "ht_index_tuple = list(zip(ht_names, ht_nodes))\n",
    "ht_index = pd.MultiIndex.from_tuples(ht_index_tuple, names=[\"HT\", \"nodes\"])\n",
    "beta_tilde = pd.DataFrame(tmp_mat, index=ht_index, columns=ht_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f\"./Results/{fitname}\"\n",
    "target_dir = Path(save_dir)\n",
    "pickle_path = Path(save_dir + \"/posteriors.pkl\")\n",
    "if not target_dir.is_dir():\n",
    "  target_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dict = dict(\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    fit=fitname,\n",
    "    fits=[fitname],\n",
    "    use_cuts=\"fromfit\",\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    theory={\"from_\": \"fit\"},\n",
    "    theoryid={\"from_\": \"theory\"},\n",
    ")\n",
    "\n",
    "# Calculate theory predictions of the input PDF\n",
    "S_dict = dict(\n",
    "    theorycovmatconfig={\"from_\": \"fit\"},\n",
    "    pdf={\"from_\": \"theorycovmatconfig\"},\n",
    "    use_t0=True,\n",
    "    datacuts={\"from_\": \"fit\"},\n",
    "    t0pdfset={\"from_\": \"datacuts\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate theory predictions of the fit with ht covmat - this will be compared to data\n",
    "preds = API.group_result_table_no_table(pdf={\"from_\": \"fit\"}, **common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_onlyreplicas = preds.iloc[:, 2:].to_numpy()\n",
    "mean_prediction = np.mean(preds_onlyreplicas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load theory covmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load power corrections covmat\n",
    "S = pd.read_csv(\n",
    "    fitpath / \"tables/datacuts_theory_theorycovmatconfig_point_prescriptions0_theory_covmat_custom_per_prescription.csv\",\n",
    "    index_col=[0, 1, 2],\n",
    "    header=[0, 1, 2],\n",
    "    sep=\"\\t|,\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "    [(aa, bb, np.int64(cc)) for aa, bb, cc in S.index],\n",
    "    names=[\"group\", \"dataset\", \"id\"],\n",
    ")\n",
    "S = pd.DataFrame(\n",
    "    S.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    ")\n",
    "S = S.reindex(C.index).T.reindex(C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load MHO covmat, if present\n",
    "try:\n",
    "  S_scale_var = pd.read_csv(\n",
    "      fitpath / \"tables/datacuts_theory_theorycovmatconfig_point_prescriptions1_theory_covmat_custom_per_prescription.csv\",\n",
    "      index_col=[0, 1, 2],\n",
    "      header=[0, 1, 2],\n",
    "      sep=\"\\t|,\",\n",
    "      engine=\"python\",\n",
    "  )\n",
    "  storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "      [(aa, bb, np.int64(cc)) for aa, bb, cc in S_scale_var.index],\n",
    "      names=[\"group\", \"dataset\", \"id\"],\n",
    "  )\n",
    "  S_scale_var = pd.DataFrame(\n",
    "      S_scale_var.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    "  )\n",
    "\n",
    "  S_scale_var = S_scale_var.reindex(C.index).T.reindex(C.index)\n",
    "  C = C + S_scale_var\n",
    "except FileNotFoundError:\n",
    "  print('No scale variations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_corr_dict = {}\n",
    "# Loop over the parameterization for the power corrections in the runcard\n",
    "for par in pc_parameters:\n",
    "    # Check that the length of shifts matches the length of nodes.\n",
    "    if len(par['yshift']) != len(par['nodes']):\n",
    "        raise ValueError(\n",
    "            f\"The length of nodes does not match that of the list in {par['ht']}.\"\n",
    "            f\"Check the runcard. Got {len(par['yshift'])} != {len(par['nodes'])}\"\n",
    "        )\n",
    "\n",
    "    # Store parameters for each power correction\n",
    "    power_corr_dict[par['ht']] = {'yshift': par['yshift'], 'nodes': par['nodes']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data_by_process = API.groups_data_by_process(**common_dict)\n",
    "pdf = API.pdf(pdf=covmat_pdf)\n",
    "shifts = {}\n",
    "for group_proc in groups_data_by_process:\n",
    "    for exp_set in group_proc.datasets:\n",
    "        if exp_set.name not in pc_excluded_exps and group_proc.name in pc_included_prcs:\n",
    "          shifts[exp_set.name] = compute_deltas_pc(exp_set, pdf, power_corr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the dataframe\n",
    "col_index = beta_tilde.droplevel('HT').index\n",
    "col_index.name = 'shifts'\n",
    "row_index = C.index\n",
    "beta = pd.DataFrame(np.zeros(shape=(row_index.size, col_index.size)), index=row_index, columns=col_index)\n",
    "beta = beta.droplevel('group')\n",
    "\n",
    "for exp_name in shifts.keys():\n",
    "  for combs_name in shifts[exp_name].keys():\n",
    "    beta.loc[(exp_name), combs_name] = shifts[exp_name][combs_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if loaded $\\beta$'s reconstruct the stored theory covmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_test = np.zeros((beta.shape[0], beta.shape[0]))\n",
    "for shift in beta.columns:\n",
    "    S_test += np.outer(beta[shift].to_numpy(), beta[shift].to_numpy())\n",
    "\n",
    "S_test = pd.DataFrame(S_test, columns=beta.index, index=beta.index)\n",
    "assert(np.allclose(S.to_numpy(), S_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum theory and experimental covmat and invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "invcov = np.linalg.inv(C + S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DataFrame containing the central values of the prior for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_ht_coeffs = pd.DataFrame(np.zeros(ht_index.shape), index=ht_index, columns=['central'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect central data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudodata = API.read_pdf_pseudodata(**common_dict)\n",
    "dat_central = np.mean(\n",
    "    [i.pseudodata.reindex(preds.index.to_list()).to_numpy().flatten() for i in pseudodata],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_tilde = TCM.construct_S_tilde(beta_tilde)\n",
    "S_hat = TCM.construct_S_hat(beta_tilde, beta)\n",
    "X = TCM.construct_X(preds_onlyreplicas, mean_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_tilde = TCM.calculate_correaltions(S_hat, invcov, X, S_tilde)\n",
    "posteriors = TCM.calculate_posterior(invcov, S_hat, mean_prediction, dat_central, central_ht_coeffs['central'])\n",
    "\n",
    "# Store in pkl\n",
    "pd.to_pickle(posteriors, save_dir + '/posteriors.pkl')\n",
    "pd.to_pickle(P_tilde, save_dir + '/P_tilde.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12413531412\n",
    "HT = HTset(posteriors, P_tilde, x_nodes, True)\n",
    "HT.generate_set(5000, seed, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ht_utlis import H2_minus_abmp, H2_plus_abmp, Ht_minus_abmp, Ht_plus_abmp, H_2_abmp, H_L, H_T_abmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pc_parameters) == 6:\n",
    "  fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(25, 25))\n",
    "  colors = [\"red\", \"green\", \"blue\", \"purple\", \"orange\", \"grey\"]\n",
    "  keys = [\"H2p\", \"HLp\", \"H2d\", \"HLd\", \"H3p\", \"H3d\"]\n",
    "elif len(pc_parameters) == 7:\n",
    "  fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(25, 25))\n",
    "  colors = [\"red\", \"green\", \"blue\", \"purple\", \"orange\", \"grey\", \"brown\"]\n",
    "  keys = [\"H2p\", \"HLp\", \"H2d\", \"HLd\", \"H3p\", \"H3d\", \"Hj\"]\n",
    "  axs[3,1].axis('off')\n",
    "else:\n",
    "  raise ValueError()\n",
    "\n",
    "color = 'lightblue'\n",
    "legends = []\n",
    "legend_names = []\n",
    "abmp = False\n",
    "\n",
    "xv = np.logspace(-5, -0.0001, 100)\n",
    "ptv = np.linspace(0.0, 3, 100)\n",
    "\n",
    "for idx_ax, (ax, key, color) in enumerate(zip(axs.flatten(), keys, colors)):\n",
    "  xaxis = xv if key != \"Hj\" else ptv\n",
    "  htf = HT(xaxis, key)\n",
    "  central = htf.mean(axis=1)\n",
    "  std = htf.std(axis=1)\n",
    "  if len(key) == 3:\n",
    "    ylabel = rf\"$H^{key[1]}_{key[2]}$\"\n",
    "    xlabel = f\"$x$\"\n",
    "  else:\n",
    "    ylabel = rf\"$H_{key[1]}$\"\n",
    "    xlabel = f\"$y$ [GeV]\"\n",
    "  \n",
    "  central_nodes = np.concatenate([HT.central_nodes.xs(level=\"HT\", key=key).to_numpy(), [0]])\n",
    "  nodes = ax.plot(x_nodes[key], central_nodes, 'o', label='data')\n",
    "  pl = ax.plot(xaxis, central, ls = \"-\", lw = 1, color = color)\n",
    "  pl_lg = ax.fill(np.NaN, np.NaN, alpha = 0.3, color = pl[0].get_color()) # Necessary for fancy legend\n",
    "  pl_fb  = ax.fill_between(xaxis, central - std, central + std , color = pl[0].get_color(),  alpha = 0.3)\n",
    "  # Zero line\n",
    "  ax.plot(xaxis, np.zeros_like(xaxis), ls = \"dashed\", lw = 2, color=\"grey\", alpha=0.6)\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.set_xlabel(xlabel, fontsize = 30)\n",
    "\n",
    "\n",
    "  ax.set_ylabel(ylabel, fontsize = 30)\n",
    "  ax.set_title(ylabel, x = 0.15, y=0.85, fontsize=30)\n",
    "  label = ylabel + rf\"$\\pm \\sigma$ MC\"\n",
    "\n",
    "  ax.legend([(pl[0], pl_lg[0]), nodes[0]], [label, \"nodes\"], loc=[0.1, 0.1], fontsize=30)\n",
    "  ax.set_xscale('log')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(save_dir + \"/ht_plots.png\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xscale('linear')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(save_dir + \"/ht_plots_linear.png\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xscale('log')\n",
    "\n",
    "if abmp:\n",
    "  axs[0][0].plot(xv, H_2_abmp(xv), ls = \"-\", lw = 1, color = 'blue', label=\"ABMP\")\n",
    "  axs[0][0].fill(np.NaN, np.NaN, color = 'blue', alpha = 0.3) # Necessary for fancy legend\n",
    "  axs[0][0].fill_between(xv, H2_plus_abmp(xv), H2_minus_abmp(xv), color = 'blue', alpha = 0.3)\n",
    "  axs[0][0].text(0.4, 0.9, \"ABMP\",transform=axs[0][0].transAxes, fontsize=40, color=\"blue\")\n",
    "\n",
    "  #HL_abmp = H_L(xv, H_2_abmp(xv), H_T_abmp(xv))\n",
    "  #axs[0][1].plot(xv, HL_abmp, ls = \"-\", lw = 1, color = 'blue')\n",
    "  #axs[0][1].fill(np.NaN, np.NaN, color = 'blue', alpha = 0.3) # Necessary for fancy legend\n",
    "  #axs[0][1].fill_between(xv,\n",
    "  #                       np.add(H2_plus_abmp(xv), -np.power(xv, 0.05) * Ht_plus_abmp(xv)),\n",
    "  #                       np.add(H2_minus_abmp(xv), -np.power(xv, 0.05) * Ht_minus_abmp(xv)),\n",
    "  #                       color = 'blue', alpha = 0.3)\n",
    "  #axs[0][1].text(0.7, 0.9, \"ABMP\",transform=axs[0][1].transAxes, fontsize=40, color=\"blue\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(save_dir + \"/comparison_abmp.png\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xscale('linear')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(save_dir + \"/comparison_abmp_linear.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_matmap = plot_covmat_heatmap(P_tilde, \"\")\n",
    "\n",
    "fig_matmap.tight_layout()\n",
    "fig_matmap.savefig(save_dir + '/heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "  def __init__(self, central_preds, covmat, nodes, label, kin_label='x'):\n",
    "    self.central_preds = central_preds\n",
    "    self.covmat = covmat\n",
    "    self.nodes = nodes\n",
    "    self.label = label\n",
    "    self.kin_label = kin_label\n",
    "\n",
    "  #def __str__(self) -> str:\n",
    "  #  return f\"{self.central:.5f} ± {self.sigma:.3f}\"\n",
    "  \n",
    "  def compute_diagonal_sigmas(self):\n",
    "    return np.sqrt(self.covmat.diagonal())\n",
    "  \n",
    "  def build_table(self, save_fig=False):\n",
    "    sigmas = self.compute_diagonal_sigmas()\n",
    "    array = np.array([f\"{central:.5f} ± {sigma:.3f}\" for central, sigma in zip(self.central_preds, sigmas)])\n",
    "    df = pd.DataFrame(array[np.newaxis, :], columns=[f\"node {i+1} | {self.kin_label} = {self.nodes[i]}\" for i in range(len(self.nodes))], index=[self.label])        \n",
    "    #df.index = [self.label]\n",
    "    fig_table, ax_posterior = plt.subplots(figsize=(10, 4))  # Set the figure size\n",
    "    ax_posterior.axis('tight')\n",
    "    ax_posterior.axis('off')  # Turn off the axis\n",
    "\n",
    "    # Create a table plot from the DataFrame\n",
    "    ax_posterior.table(cellText=df.values, colLabels=df.columns, rowLabels=df.index, loc='center')\n",
    "\n",
    "    fig_table.tight_layout()\n",
    "    if save_fig:\n",
    "      save_dir_tables = Path(f\"{save_dir}/tables\")\n",
    "      if not save_dir_tables.is_dir():\n",
    "        save_dir_tables.mkdir(parents=True, exist_ok=True)\n",
    "      fig_table.savefig(save_dir_tables / f\"{self.label}_table.png\")\n",
    "  \n",
    "    return fig_table\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_H2p = Prediction(posteriors.xs(\"H2p\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"H2p\", level=\"HT\").T.xs(\"H2p\", level=\"HT\").to_numpy(), x_nodes['H2p'], 'H2p', 'x')\n",
    "pred_HLp = Prediction(posteriors.xs(\"HLp\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"HLp\", level=\"HT\").T.xs(\"HLp\", level=\"HT\").to_numpy(), x_nodes['HLp'], 'HLp', 'x')\n",
    "pred_H3p = Prediction(posteriors.xs(\"H3p\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"H3p\", level=\"HT\").T.xs(\"H3p\", level=\"HT\").to_numpy(), x_nodes['H3p'], 'H3p', 'x')\n",
    "pred_H2d = Prediction(posteriors.xs(\"H2d\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"H2d\", level=\"HT\").T.xs(\"H2d\", level=\"HT\").to_numpy(), x_nodes['H2d'], 'H2d', 'x')\n",
    "pred_HLd = Prediction(posteriors.xs(\"HLd\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"HLd\", level=\"HT\").T.xs(\"HLd\", level=\"HT\").to_numpy(), x_nodes['HLd'], 'HLd', 'x')\n",
    "pred_H3d = Prediction(posteriors.xs(\"H3d\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"H3d\", level=\"HT\").T.xs(\"H3d\", level=\"HT\").to_numpy(), x_nodes['H3d'], 'H3d', 'x')\n",
    "try:\n",
    "  pred_Hj = Prediction(posteriors.xs(\"Hj\", level=\"HT\").to_numpy().reshape((-1)), P_tilde.xs(\"Hj\", level=\"HT\").T.xs(\"Hj\", level=\"HT\").to_numpy(), x_nodes['Hj'], 'Hj', 'pT')\n",
    "except KeyError:\n",
    "  print('No power corrections for jets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_H2p.build_table(True)\n",
    "pred_HLp.build_table(True)\n",
    "pred_H3p.build_table(True)\n",
    "pred_H2d.build_table(True)\n",
    "pred_HLd.build_table(True)\n",
    "pred_H3d.build_table(True)\n",
    "try:\n",
    "  pred_Hj.build_table(True)\n",
    "except NameError:\n",
    "  print('No power corrections for jets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
